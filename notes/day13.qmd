---
title: Eigenvalues and Eigenvectors
format: beamer
author: Jeremy Teitelbaum
jupyter:
    kernelspec:
        name: "base"
        language: "python"
        display_name: "base"
---

## Eigenvalues and Eigenvectors

If $A$ is a diagonal matrix:
$$
A=\left[\begin{matrix} 2 & 0 \\ 0 & 1/3\end{matrix}\right]
$$
then the linear transformation $x\mapsto Ax$ "stretches" along the $x$-axis and "shrinks" along the $y$-axis.

```{python}
import matplotlib.pyplot as plt

# Define the vertices of the unit square centered at the origin
square_vertices = [
    [-0.5, -0.5],
    [0.5, -0.5],
    [0.5, 0.5],
    [-0.5, 0.5],
    [-0.5, -0.5],  # Close the square
]

# Define the vertices of the rectangle centered at the origin
rectangle_vertices = [
    [-1, -1 / 6],
    [1, -1 / 6],
    [1, 1 / 6],
    [-1, 1 / 6],
    [-1, -1 / 6],  # Close the rectangle
]

# Extract x and y coordinates for the square
x_coords_square, y_coords_square = zip(*square_vertices)

# Extract x and y coordinates for the rectangle
x_coords_rectangle, y_coords_rectangle = zip(*rectangle_vertices)

# Create the plot with two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

# Plot the unit square
ax1.plot(x_coords_square, y_coords_square, "b-")  # Draw the square with blue lines
ax1.axhline(0, color="black", linewidth=0.5)
ax1.axvline(0, color="black", linewidth=0.5)
ax1.set_aspect("equal", adjustable="box")
ax1.set_xlim([-3, 3])
ax1.set_ylim([-3, 3])
ax1.set_xlabel("X axis")
ax1.set_ylabel("Y axis")
ax1.set_title("Unit Square (Before x->Ax)")

# Plot the rectangle
ax2.plot(
    x_coords_rectangle, y_coords_rectangle, "b-"
)  # Draw the rectangle with blue lines
ax2.axhline(0, color="black", linewidth=0.5)
ax2.axvline(0, color="black", linewidth=0.5)
ax2.set_aspect("equal", adjustable="box")
ax2.set_xlim([-3, 3])
ax2.set_ylim([-3, 3])
ax2.set_xlabel("X axis")
ax2.set_ylabel("Y axis")
ax2.set_title("Rectangle (after x->Ax)")

# Display the plot
plt.tight_layout()
plt.show()
```

## Eigenvalues and Eigenvectors

If $A$ is upper triangular, say
$$
A=\left[\begin{matrix} 2 & 1 \\ 0 & 1/3\end{matrix}\right]
$$
then $A$ stretches along the $x$-axis by $2$ as before.  Less obviously, it shrinks along the direction given by the vector $(-3/5,1)$.

```{python}
import matplotlib.pyplot as plt

# Define the vertices of the first parallelogram centered at the origin
parallelogram1_vertices = [
    [0, 0],
    [1, 0],
    [1 - 3/5, 1],
    [-3/5, 1],
    [0, 0]  # Close the parallelogram
]

# Define the vertices of the second parallelogram centered at the origin
parallelogram2_vertices = [
    [0, 0],
    [2, 0],
    [2 - 1/5, 1/3],
    [-1/5, 1/3],
    [0, 0]  # Close the parallelogram
]

# Extract x and y coordinates for the first parallelogram
x_coords_parallelogram1, y_coords_parallelogram1 = zip(*parallelogram1_vertices)

# Extract x and y coordinates for the second parallelogram
x_coords_parallelogram2, y_coords_parallelogram2 = zip(*parallelogram2_vertices)

# Create the plot with two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

# Plot the first parallelogram
ax1.plot(x_coords_parallelogram1, y_coords_parallelogram1, 'b-')  # Draw the parallelogram with blue lines
ax1.axhline(0, color='black', linewidth=0.5)
ax1.axvline(0, color='black', linewidth=0.5)
ax1.set_aspect('equal', adjustable='box')
ax1.set_xlim([-3, 3])
ax1.set_ylim([-3, 3])
ax1.set_xlabel('X axis')
ax1.set_ylabel('Y axis')
ax1.set_title('Parallelogram with Vectors (1,0) and (-3/5,1)')

# Plot the second parallelogram
ax2.plot(x_coords_parallelogram2, y_coords_parallelogram2, 'b-')  # Draw the parallelogram with blue lines
ax2.axhline(0, color='black', linewidth=0.5)
ax2.axvline(0, color='black', linewidth=0.5)
ax2.set_aspect('equal', adjustable='box')
ax2.set_xlim([-3, 3])
ax2.set_ylim([-3, 3])
ax2.set_xlabel('X axis')
ax2.set_ylabel('Y axis')
ax2.set_title('Parallelogram with Vectors (2,0) and (-1/5,1/3)')

# Display the plot
plt.tight_layout()
plt.show()
```

## Eigenvalues and Eigenvectors

An **eigenvector** for a matrix $A$ is a vector $v$ which gets shrunk or lengthened by $A$ by some factor $\lambda$.  

The factor $\lambda$ is called the **eigenvalue**.

More formally, a vector $v$ is called an eigenvector for $A$ (with eigenvalue $\lambda$) if $v$ is not zero and
$$
Av=\lambda v.
$$

## Eigenvalues and Eigenvectors

In the example above, the vectors $\left[\begin{matrix} 1 \\ 0\end{matrix}\right]$ and $\left[\begin{matrix} -3/5 \\ 1\end{matrix}\right]$ are eigenvectors for the matrix
$$
A = \left[\begin{matrix} 2 & 1 \\ 0 & 1/3\end{matrix}\right]
$$
with eigenvalues $2$ and $1/3$ respectively.
$$
 \left[\begin{matrix} 2 & 1 \\ 0 & 1/3\end{matrix}\right]\left[\begin{matrix} 1 \\ 0\end{matrix}\right] = 2\left[\begin{matrix} 1 \\ 0\end{matrix}\right]
$$
$$
\left[\begin{matrix} 2 & 1 \\ 0 & 1/3\end{matrix}\right]\left[\begin{matrix} -3/5 \\ 1\end{matrix}\right] = \left[\begin{matrix} -1/5 \\ 1/3\end{matrix}\right]=(1/3)\left[\begin{matrix} -3/5 \\ 1\end{matrix}\right]
$$

## Triangular Matrices

If $A$ is (upper) triangular then the diagonal entries for $A$ are all eigenvalues.  If the diagonal entries are distinct, thenhere are $n$ linearly independent eigenvectors.

## Eigenspaces

Suppose that $\lambda$ is a constant.  The vectors $v$ such that 
$$
Av=\lambda v
$$
form a subspace called the *eigenspace* for $\lambda$. 

This subspace is the nullspace of the matrix
$$
A-\lambda I_{n}
$$
where $I_{n}$ is the $n\times n$ identity matrix.

## Independence of Eigenvectors

If $v_1,\ldots, v_n$ are eigenvectors for a matrix $A$ with eigenvalues $\lambda_1,\ldots,\lambda_n$, and all the $\lambda_i$ are different, then the $v_{i}$ are linearly independent. (Note that the $v_i$ are nonzero.)

To see this, suppose that 
$$
c_1 v_1 + c_2 v_2 + \cdots c_n v_n = 0.
$$

Then 
$$
A(c_1 v_1 + c_2 v_2 + \cdots c_n v_n)=c_1\lambda_1 v_1 + \cdots c_n\lambda_n v_n=0
$$

Multiply the first relation by $\lambda_1$ and subtract.  You get
$$
c_2(\lambda_2-\lambda_1)v_2 + \cdots + c_n(\lambda_n-\lambda_1)v_n=0.
$$
Since the differences of the $\lambda_i$ with $\lambda_1$ are not zero, we see that $v_2,\ldots, v_n$
are dependent.

By repeating this you can show that smaller and smaller collections of the $v_i$ are dependent until you ultimately get $v_n=0$.  

## Characteristic Equation

Finding eigenvalues and eigenvectors of a matrix is a hard problem.  We can make the following observation.

Suppose $\lambda$ is an eigenvalue of $A$ where $A$ is an $n\times n$ matrix.   Then there is a vector $v\not=0$ so that $Av=\lambda v$.
This means that the matrix
$A-\lambda I_n$ is *not* invertible because $v$ is in its null space.  

As a result, $\det(A-\lambda I_n=0$.  

Conversely, if $\det(A-\lambda I_n)=0$, then there is a vector $v$ in the null space and that $v$ is an eigenvector.

It turns out that $\det(A-\lambda I_n)$ is a polynomial in $\lambda$, so the eigenvalues of $A$ are the roots of this polynomial.

## Example

Let 
$$
A=\left[\begin{matrix} 3 & 5 \\ 2 & 4\end{matrix}\right].
$$

The determinant of $A-\lambda$ is
$$
\det(\left[\begin{matrix} 3-\lambda & 5\\2 & 4-\lambda\end{matrix}\right])=(3-\lambda)(4-\lambda)-10
$$

The polynomial on the right is
$$
(3-\lambda)(4-\lambda)-10=\lambda^2-7\lambda+12-10=\lambda^2-7\lambda+2.
$$

Its roots are $\frac{7\pm\sqrt{41}}{2}$. These are the eigenvalues of $A$; they are approximately
$6.70156$ and $0.29843$.


## Example continued

To find the eigenvectors, we have to compute the null space of $A-\lambda I$.  This is no fun
algebraically but with some work you find that the eigenvectors are:

```{python}
from sympy import Matrix, latex
from IPython.display import Latex
M = Matrix([[3,5],[2,4]])
L=M.eigenvects()
display(Latex(f"$${latex(L[0][2][0])}$$ and $${latex(L[1][2][0])}$$"))
```

## Bigger matrices

The characteristic polynomial of an $n\times n$ matrix is of degree $n$.

**Theorem:** A polynomial of degree $n$ has $n$ complex roots (counted correctly).

However, in practice one must use numerical methods to find roots of a polynomial of degree 3 or higher. 

If one know an eigenvalue $\lambda$ *exactly* (which usually doesn't happen) then you can find the eigenvectors by computing the null space
of $A-\lambda$.

## Example

Let 
$$
A=\left[\begin{matrix} 5 & -8 & 1 \\ 0& 0 &7 \\ 0 & 0 & -2\end{matrix}\right]
$$

Since $A$ is (lower) triangular, the eigenvalues are $5$,$0$ and $-2$.
The vector
$$
\left[\begin{matrix} 1 \\ 0 \\ 0\end{matrix}\right]
$$ 
is the eigenvector with eigenvalue $5$.

## Example continued

To find the eigenvector with eigenvalue $0$, we must solve
$$
Ax = 0.
$$

By one of our techniques this is
$$
\left[\begin{matrix} -8 \\ 5 \\ 0\end{matrix}\right]
$$

## Example continued

The final eigenvector is in the null space of $A+2I$:
$$
A=\left[\begin{matrix} 7 & -8 & 1 \\ 0& 2 &7 \\ 0 & 0 & 0\end{matrix}\right]
$$

This is
$$
\left[\begin{matrix}-58/7 \\ -7 \\ 2\end{matrix}\right]
$$

## Similarity

Two (square) matrices $A$ and $B$ are *similar* if there is an invertible matrix $P$ so that 
$A=PBP^{-1}$.

Similar matrices have the same eigenvalues because they have the same characteristic polynomial.

$$
\begin{array}{rcl}
\det(PBP^{-1}-\lambda I) &=& \det(P(B-\lambda I)P^{-1})\\& =& \det(P)\det(B-\lambda I)\det(P^{-1})\\ &=&\det(B-\lambda I)
\end{array}
$$

## Diagonalization

Diagonal matrices are the simplest to work with.

**Definition:** A square matrix is diagonalizable if there is an invertible matrix $P$ so that $A=PDP^{-1}$ where $P$ diagonal.  In other words, $A$ is diagonalizable if it is similar to a diagonal matrix. 

**Theorem:** An $n\times n$ matrix $A$ is diagonalizable if and only if it has $n$ linearly independent eigenvectors.

In fact, if $A=PDP^{-1}$, then the columns of $P$ are $n$ linearly independent eigenvectors for $A$, and the diagonal entries of $D$ are the corresponding eigenvalues.  This is because in this situation,
$$
AP = PD.
$$

## Example (from the text)

Let 
$$
A=\left[\begin{matrix} 1 & 3 & 3\\-3 & -5 & -3\\3 & 3 & 1\end{matrix}\right]
$$

1. The eigenvalues of $A$.  The text tells us that the characteristic polynomial of $A$ is $-(\lambda-1)(\lambda+2)^2$ so the eigenvalues are $1$ and $-2$. 

## Example continued

We need three linearly independent eigenvectors.  So we need the null spaces of $A-I$ and $A+2I$.  The book gives us:
$$
v_1=\left[\begin{matrix} 1 \\ -1 \\ 1\end{matrix}\right]
$$
with eigenvalue $1$.  For the eigenvalue $-2$:

```{python}
from sympy import Matrix, latex, eye
from IPython.display import Latex

A = Matrix([[1, 3, 3], [-3, -5, -3], [3, 3, 1]])
display(Latex(f"A+2I=${latex(A+2*eye(3))}$"))
display(Latex(f"Reduced form is ${latex((A+2*eye(3)).rref()[0])}$"))
```

## Example continued
So null space of $A+2I$ is two dimensional and spanned by
$$
\left[\begin{matrix}1 \\ -1 \\0 \end{matrix}\right],\left[\begin{matrix} 1 \\ 0 \\ -1\end{matrix}\right]
$$

Therefore
$$
P = \left[\begin{matrix} 1 & 1 & 1\\ -1 & -1 & 0 \\ 1 & 0 & -1\end{matrix}\right]
$$

## Example continued

We can compute $AP$:
```{python}
P=Matrix([[1,1,1],[-1,-1,0],[1,0,-1]])
display(Latex(f"AP=${latex(A@P)}$=PD"))
```
where
$$
D=\left[\begin{matrix} 1 & 0 & 0\\0 & 2 & 0 \\ 0 & 0 & 2\end{matrix}\right]
$$

So $A=PDP^{-1}$ and $A$ is diagonalizable.

## Matrix Powers

One application of diagonalization is that it makes it feasible to understand $A^{n}$
when $A$ is a square matrix. 

If $A$ is diagonalizable, then there is a diagonal matrix $D$ and a matrix $P$ so that
$$
A=PDP^{-1}.
$$

Then 
$$
A^{m}=(PDP^{-1})(PDP^{-1})\cdots(PDP^{-1})=PD^{m}P^{-1}
$$
and
$$
D^{n} = \left[\begin{matrix} \lambda_{1}^{n} & 0 & \cdots & 0 \\ 0 & \lambda_{2}^{n} & \cdots & 0 \\
\vdots & \vdots & \ddots & 0\\
0 & 0 & \cdots & \lambda_{n}^{m}\end{matrix}\right]
$$


## Not all matrices are diagonalizable

Let 
$$
A=\left[\begin{matrix} 1 & 1 \\ 0 & 1\end{matrix}\right].
$$
The only eigenvalue is $1$.  The null space of $A-I$ is
only one dimensional, spanned by 
$$
\left[\begin{matrix} 1\\ 0\end{matrix}\right]
$$

So there's no basis of eigenvectors, so $A$ can't be diagonalized.

## $n$ distinct eigenvalues implies diagonalizable
If $A$ has $n$ different eigenvalues, then it has $n$ linearly independent 
eigenvectors; thus there is a basis of eigenvectors.

Therefore in this case $A$ is diagonalizable.

But as we saw above, you can have repeated eigenvalues and still be diagonalizable. 

## The diagonalization Theorem

![Diagonalization Theorem](./diag_theorem.png)

## Fibonacci Numbers

The Fibonacci numbers are defined recursively  by $F_{0}=0$, $F_{1}=1$, and $F_{n} = F_{n-1}+F_{n-2}$ for
$n>1$.

Let 
$$
A =\left[\begin{matrix} 0 & 1 \\ 1 & 1\end{matrix}\right]=\left[\begin{matrix} F_{0} & F_{1}\\F_{1} & F_{2}\end{matrix}\right].
$$

Then 
$$
A^{2} = AA = \left[\begin{matrix} 1 & 1 \\ 1 & 2\end{matrix}\right]=\left[\begin{matrix} F_{1} & F_{2}\\F_{2} & F_{3}\end{matrix}\right]
$$

## Fibonacci continued
Continuing we see that

\begin{multline}
A^{n} = AA^{n-1} = A\left[\begin{matrix} F_{n-1} & F_{n}\\F_{n} & F_{n+1}\end{matrix}\right]=\\
\left[\begin{matrix} F_{n} & F_{n+1} \\ F_{n-1}+F_{n} & F_{n}+F_{n+1}\end{matrix}\right]=\\
\left[\begin{matrix}
F_{n} & F_{n+1} \\ F_{n+1} & F_{n+2}\end{matrix}\right]
\end{multline}

So:
$$
A^{n}=\left[\begin{matrix} F_{n} & F_{n+1} \\ F_{n+1} & F_{n+2}\end{matrix}\right]
$$

## Computation of Fibonacci numbers

Let's try to diagonalize the matrix $A=\left[\begin{matrix} 0 & 1 \\ 1 & 1 \end{matrix}\right]$ and use this to compute $A^{n}$. 

The characteristic polynomial of $A$:
$$
F(\lambda)=\det(A-\lambda I)=\det\left[\begin{matrix} -\lambda & 1\\ 1 & 1-\lambda\end{matrix}\right]
$$
so $F(\lambda)=\lambda^2-\lambda-1.$  This polynomial has two roots:
$$
\lambda_{\pm} = \frac{1\pm\sqrt{5}}{2}.
$$

Therefore $A$ is diagonalizable.  To compute $A^{n}$ we need to find $P$ so that
$$
A = P\left[\begin{matrix} \lambda_{+} & 0 \\ 0 & \lambda_{-}\end{matrix}\right]P^{-1}
$$

## Computation of Fibonacci Numbers continued

The columns of the matrix $P$ are the eigenvectors of $A$.  To find these we must solve
$$
A\left[\begin{matrix} x \\ y\end{matrix}\right]=\left[\begin{matrix}\lambda_{\pm} x & \lambda_{\pm}y\end{matrix}\right]
$$
which translates into the equations
$$
\left[\begin{matrix} y \\ x+y \end{matrix}\right]=\left[\begin{matrix}\lambda_{\pm}x\\ \lambda_{\pm}y\end{matrix}\right]
$$

Eigenvectors are determined only up to scaling so we can set $x=1$.  Then $y=\lambda_{\pm}$. So 
$$
P=\left[\begin{matrix} 1 & 1 \\ \lambda_{+} & \lambda_{-}\end{matrix}\right]
$$


## Fibonacci numbers 

Our final result is that
$$
\left[\begin{matrix} F_{n-1} & F_{n} \\ F_{n} & F_{n+1}\end{matrix}\right]=A^{n} = \frac{1}{\lambda_{-}-\lambda_{+}}\left[\begin{matrix} 1 & 1 \\\lambda_{+} & \lambda_{-}\end{matrix}\right]
\left[\begin{matrix}\lambda_{+}^{n} & 0 \\ 0 & \lambda_{-}^{n}\end{matrix}\right]\left[\begin{matrix}\lambda_{-} & -1 \\ -\lambda_{+} & 1 \end{matrix}\right]
$$

A little algebra gives:
$$
F_{n} = \frac{(\frac{1+\sqrt{5}}{2})^n-(\frac{1-\sqrt{5}}{2})^n}{\sqrt{5}}
$$

## Consequences

Let $\phi$ be the "Golden ratio" $\frac{1+\sqrt{5}}{2}$. 

1.  $F_{n}$ is approximately $\phi^{n}/\sqrt{5}$. 
2.  $F_{n}/F_{n-1}$ converges to $\phi$.

```{python}
import numpy as np
import matplotlib.pyplot as plt

f, fx = 0, 1
phi = (1 + np.sqrt(5)) / 2
phibar = (1 + np.sqrt(5)) / 2
y1 = []
y2 = []

for i in range(12):
    fphi = phi ** (i) / np.sqrt(5)
    print(f"{i}\t\t{fphi:.3f}\t\t{f}")
    f, fx = fx, f + fx
```

