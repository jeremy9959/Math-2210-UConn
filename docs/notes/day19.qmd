---
title: Diagonalization of Symmetric Matrices
author: Jeremy Teitelbaum
format: beamer
---

## Symmetric Matrices

A matrix $A$ is *symmetric* if $A^{T}=A$.  For example
$$
A =\left[\begin{matrix}1 & 3 & 1 \\ 3 & -5 & 7 \\ 1 & 7 & 2 \end{matrix}\right]
$$
is symmetric.

In our discussion of least squares fitting, we ended up with  the matrix $X^{T}X$ 
where $X$ was our data matrix.  This matrix is symmetric because $(X^{T}X)^{T}=X^{T}X$.

If $U$ is a matrix is an $n\times m$ matrix with columns $u_{i}$ for $i=1,\ldots,m$, then $U^{T}U$ is an $m\times m$ matrix whose $i,j$ entry is the dot product of $u_i$ and $u_j$.  It is symmetric because the dot product is commutative.

## Eigenvalues/vectors of symmetric matrices

Eigenvalues and vectors of symmetric matrices have special properties.
Suppose $A$ is an $n\times n$ symmetric matrix.

Theorem: If $v$ and $w$ are eigenvectors of $A$ with eigenvalues $\lambda$ and $\mu$ where $\lambda\not=\mu$ then $v\cdot w=0$. 

To see this, consider $Av=\lambda v$ and $Aw=\mu w$.  Then $w^{T}A^{T}=\mu w^{T}$.
But $A^{T}=A$ wo $w^{T}A=\mu w^{T}$.  

Remember that $w^{T}v=w\cdot v$. 

This gives us
$$
w^{T}Av=\mu(w^{T}v) = \mu(w\cdot v) = \lambda(w^{T}v)=\lambda(w\cdot v)
$$

Since $\lambda\not=\mu$ this means $w\cdot v=0$. 

## Orthogonal Diagonalizability

Remember that a square matrix $A$ is diagonalizable if there is a matrix $P$ so that
$A=PDP^{-1}$ where $D$ is a diagonal matrix (whose entries are the eigenvalues of $A$).

**Definition:** An $n\times n$ matrix $A$ is *orthogonally diagonalizable* if is an
orthonormal set $u_1,\ldots, u_n$ of eigenvectors of $A$.

In other words, $A$ has $n$ mutually orthogonal eigenvectors of length $1$.

## Orthogonal Diagonalizability example

Let 
$$
A = \left[\begin{matrix} 1 & 2 \\ 2 & 1 \end{matrix}\right]
$$

Note that $A$ is symmetric.  The characteristic polynomial of $A$ is
$$
T^2-2T-3=(T-3)(T+1)
$$
so the eigenvalues are $3$ and $-1$.

The eigenvectors are 
$$
v_1=\left[\begin{matrix} 1 \\ 1\end{matrix}\right],v_2=\left[\begin{matrix} -1 \\ 1 \end{matrix}\right].
$$


## Orthogonal diagonalizabiilty example (cont'd)

Notice that $v_1$ and $v_2$ are orthogonal.  To make them orthonormal, divide by their norms to get
$$
u_1=\left[\begin{matrix} \sqrt{2}/2 \\ \sqrt{2}/2\end{matrix}\right],u_2 = \left[\begin{matrix} -\sqrt{2}/{2}\\\sqrt{2}/{2}\end{matrix}\right]
$$

We therefore have the equation:
$$
A\left[\begin{matrix} \sqrt{2}/2 & -\sqrt{2}/2 \\ \sqrt{2}/2 & \sqrt{2}/2\end{matrix}\right]=\left[\begin{matrix} \sqrt{2}/2 & -\sqrt{2}/2 \\ \sqrt{2}/2 & \sqrt{2}/2\end{matrix}\right]\left[\begin{matrix} 3 & 0 \\ 0 & -1\end{matrix}\right]
$$

and
$$
A = PDP^{-1}
$$
where
$$
P = \left[\begin{matrix} \sqrt{2}/2 & -\sqrt{2}/2 \\ \sqrt{2}/2 & \sqrt{2}/2\end{matrix}\right], D=\left[\begin{matrix} 3 & 0 \\ 0 & -1\end{matrix}\right]
$$

## Orthogonal diagonalizabiilty example (cont'd)

The matrix $P$ is an *orthogonal* matrix:

- its columns are an orthonormal set.
- $P^T=P^{-1}$

## The Spectral Theorem

Symmetric  matrices are *special.*  

![Spectral Theorem](./spectral_thm.png)

## Spectral Decomposition

Let $u_1,\ldots, u_n$ be the orthonormal eigenvectors of $A$, where $A$ is symmetric.
Let $\lambda_{1},\ldots, \lambda_{n}$ be the eigenvalues (with multiplicity)

Then
$$
A = \lambda_{1} u_{1}u_{1}^{T} + \cdots + \lambda_{n}u_{n}u_{n}^{T}
$$

This writes $A$ as a (weighted) sum of projection operators since $u_{i}u_{i}^{T}v$
gives the projection of $v$ into the $u$ direction.

## Spectral Decomposition example

In our $2\times 2$ example we had 
$$
u_1 =\left[\begin{matrix} \sqrt{2}/2 \\ \sqrt{2}/2\end{matrix}\right],
u_2 = \left[\begin{matrix}-\sqrt{2}/2 \\ \sqrt{2}/2\end{matrix}\right]
$$

So 
$$
u_{1}u_{1}^{T} =\left[\begin{matrix} \sqrt{2}/2 \\ \sqrt{2}/2\end{matrix}\right]\left[\begin{matrix} \sqrt{2}/2 & \sqrt{2}/2 \end{matrix}\right] =
\left[\begin{matrix} 1/2 & 1/2 \\ 1/2 & 1/2 \end{matrix}\right]
$$

and

$$
u_{2}u_{2}^{T} = \left[\begin{matrix} -\sqrt{2}/2 \\ \sqrt{2}/2\end{matrix}\right]
\left[\begin{matrix} -\sqrt{2}/2 & \sqrt{2}/2 \end{matrix}\right] = \left[\begin{matrix} 1/2 & -1/2 \\ -1/2 & 1/2\end{matrix}\right]
$$

And

$$
A=3\left[\begin{matrix} 1/2 & 1/2 \\ 1/2 & 1/2\end{matrix}\right] -\left[\begin{matrix} 1/2 & -1/2 \\ -1/2 & 1/2\end{matrix}\right]
$$

##
